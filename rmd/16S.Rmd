---
title: "16S"
author: "Nicholas Baetge"
date: "8/13/2020"
output: github_document
---

# Intro

Here, the NAAMES cast 16S sequences processed by Luis Bolanos in the Giovannoni group are explored

```{r message = F, warning = F}
library(tidyverse) 
library(rmarkdown)
library(knitr)
library(readxl)
library(data.table) 
library(scales)
library(zoo)
library(oce)
library(patchwork)
#rmarkdown tables
library(stargazer)
library(pander)
#stat tests
library(lmtest)
library(lmodel2)
library(rstatix)
library(ggpubr)
#for odv type plots
library(lubridate)
library(reshape2)
library(MBA)
library(mgcv)
#phyloseq
library(phyloseq)
library(RColorBrewer)


```
```{r}
custom_theme <- function() {
  theme_test(base_size = 30) %+replace%
    theme(legend.position = "right",
          legend.spacing.x = unit(0.5,"cm"),
          legend.title = element_text(size = 14),
          legend.text = element_text(size = 14),
          legend.background = element_rect(fill = "transparent",colour = NA),
          legend.key = element_rect(fill = "transparent",colour = NA),
          panel.background = element_rect(fill = "transparent",colour = NA),
          plot.background = element_rect(fill = "transparent",colour = NA)) 
}

custom.colors <- c("AT39" = "#377EB8", "AT34" = "#4DAF4A", "AT38" = "#E41A1C", "AT32" = "#FF7F00", "Temperate" = "#A6CEE3", "Subpolar" = "#377EB8", "Subtropical" = "#FB9A99", "GS/Sargasso" = "#E41A1C", "Early Spring" = "#377EB8", "Late Spring" = "#4DAF4A","Early Autumn" = "#E41A1C", "Summer" = "#E41A1C", "Late Autumn" = "#FF7F00", "Gv2_2019" = "#377EB8", "WOA18_MN" = "#4DAF4A", "WOA18_AN" = "#E41A1C")

levels = c("GS/Sargasso", "Subtropical", "Temperate", "Subpolar",  "AT39-6", "AT34", "AT38", "AT32","South", "North", "Early Spring", "Late Spring","Early Autumn",  "Summer", "Late Autumn", "Gv2_2019", "WOA18_MN", "WOA18_AN","Nov", "Nov sd", "Dec", "Dec sd", "Jan", "Jan sd", "Feb", "Feb sd", "Mar", "Mar sd", "Apr", "Apr sd",  "Cruise", "ARGO", "5-100 m", "150-200 m", "300 m", "> 300 m", "0", "1", "2", "3")


bar.colors <- c("100 m" = "white", "CM" = "#4DAF4A",  "PAM" = "#377EB8")

odv.colors <- c("#feb483", "#d31f2a", "#ffc000", "#27ab19", "#0db5e6", "#7139fe", "#d16cfa")
```

# Import Data 

```{r message = F}

count.tab <- read.table("~/GITHUB/naames_multiday/Input/16s/HetV1OTU.txt", header = T, row.names = 1, check.names = F)

tax.tab <- as.matrix(read.table("~/GITHUB/naames_multiday/Input/16s/HetV1TUtax.txt", header = T, row.names = 1, check.names = F, na.strings = "", sep = "\t"))


ctd <-  readRDS("~/GITHUB/naames_multiday/Input/ctd/deriv_naames_ctd.rds") %>%
              select(Cruise, Station, CampCN,  bin_depth, deriv_o2_umol_l, fl_mg_m3, ave_temp_c, ave_sal_psu) %>% 
              mutate(Cruise = ifelse(Cruise == "AT39", "AT39-6", Cruise)) %>% 
              rename(z = bin_depth,
                     o2 = deriv_o2_umol_l,
                     fl = fl_mg_m3,
                     temp = ave_temp_c,
                     sal = ave_sal_psu) 


npp <- read_rds("~/GITHUB/naames_multiday/Input/Z_resolved_model_NPP.rds") %>% 
  rename(z = depth,
         npp = NPP)

sample.tab <- read_rds("~/GITHUB/naames_multiday/Input/export_ms/processed_bf.2.2020.rds") %>% 
  select(Cruise:CampCN, Target_Z, DNA_ID) %>% 
  drop_na(DNA_ID) %>% 
  rename(z = Target_Z) %>% 
  left_join(., read_rds("~/GITHUB/naames_multiday/Output/processed_data.rds") %>%
              select(Cruise, Station, Date,  CampCN, mld, z,  doc, n, phyc, bc, bcd ) %>% 
              distinct() %>% 
              mutate_at(vars(phyc:bcd), function(x)(x/10^3))) %>% 
  mutate(`Depth Interval` = ifelse(z <= 100, "5-100 m", "150-200 m"),
         `Depth Interval` = ifelse(z == 300, "300 m", `Depth Interval`),
         `Depth Interval` = ifelse(z > 300, "> 300 m", `Depth Interval`)) %>% 
  select(Cruise:z, `Depth Interval`, everything()) %>% 
  left_join(., ctd) %>% 
  left_join(., npp) %>% 
  rename(latitude = Latitude) %>% 
  column_to_rownames(var = "DNA_ID") 

```

# Phyloseq Object

We need to create a phyloseq object that merges all three datasets. Sometimes this doesn't work beacuse of the format of the data files. Make sure all the sample names between the sampleinfo.txt and seqtab-nochimtaxa.txt are the same

```{r}
OTU = otu_table(count.tab, taxa_are_rows = TRUE) 
TAX = tax_table(tax.tab)
SAM = sample_data(sample.tab)
ps = phyloseq(OTU,TAX,SAM) 

sample_data(ps)$`Depth.Interval` <- factor(sample_data(ps)$`Depth.Interval`, levels = levels)
sample_data(ps)$Season <- factor(sample_data(ps)$Season, levels = levels)

```

# Filter sequences

We will filter out  chloroplasts and mitochondria, because we only intended to amplify bacterial sequences. It's good to check you don’t have anything lurking in the taxonomy table. 

```{r}
sub_ps <- ps %>%
  subset_samples(z %in% c(5, 25, 50, 75, 100, 150, 200, 300)) %>%
  subset_taxa(
    Family  != "mitochondria" &
    Class   != "Chloroplast"
  )


```

# Sample Summary

As a first analysis, we will look at the distribution of read counts from our samples

```{r echo = FALSE, warning = FALSE, message = FALSE, fig.height = 6, fig.width = 12, fig.align = "center"}
# Make a data frame with a column for the read counts of each sample
sample_sum_df <- data.frame(sum = sample_sums(sub_ps))

# Histogram of sample read counts
ggplot(sample_sum_df, aes(x = sum)) + 
  geom_histogram(color = "black", fill = "#377EB8", binwidth = 2500) +
  ggtitle("Distribution of sample sequencing depth") + 
  xlab("Read counts") +
  theme(axis.title.y = element_blank()) +
  custom_theme()
```

```{r}
# mean, max and min of sample read counts
smin <- min(sample_sums(sub_ps)) #3944
smean <- mean(sample_sums(sub_ps)) #85824.59
smax <- max(sample_sums(sub_ps)) #251468
```

# Stacked Barplots

Let’s make a stacked barplot of Phyla to get a sense of the community composition in these samples.

Since this is not a quantitative analysis, and since we have more Phyla in this dataset than we can reasonably distinguish colors, we will prune out low abundance taxa and only include Phyla that contribute more than 2% of the relative abundance of each sample. Depending on your dataset and the taxonomic level you are depicting, you can adjust this prune parameter. In later analyses, we will of course included these taxa, but for now they will just clutter our plot.


```{r}
# melt to long format (for ggploting) 
# prune out phyla below 2% in each sample

sub_ps_phylum <- sub_ps %>%
  tax_glom(taxrank = "Phylum") %>%                     # agglomerate at phylum level
  transform_sample_counts(function(x) {x/sum(x)} ) %>% # Transform to rel. abundance
  psmelt() %>%                                         # Melt to long format
  filter(Abundance > 0.01) %>%                         # Filter out low abundance taxa
  arrange(Phylum)                                      # Sort data frame alphabetically by phylum
```

```{r echo = FALSE, warning = FALSE, message = FALSE, fig.height = 16, fig.width = 18, fig.align = "center"}

phylum.colors <- colorRampPalette(brewer.pal(10, "Accent"))(length(unique(sub_ps_phylum$Phylum)))

# Plot 
ggplot(sub_ps_phylum, aes(x = latitude, y = Abundance, fill = Phylum)) + 
  facet_grid(z~Season) +
  geom_bar(stat = "identity", position = "stack", width = 0.5, alpha = 0.7) +
  scale_fill_manual(values = phylum.colors) +
  labs(x = "Latitude, ˚N") +
  custom_theme() +
  guides(fill = guide_legend(reverse = TRUE, keywidth = 1, keyheight = 1)) +
  ylab("Relative Abundance (Phyla > 1%) \n") +
  ggtitle("Phylum Composition of Bacterial Communities")
```
# Beta Diversity

Beta diversity involves calculating metrics such as distances or dissimilarities based on pairwise comparisons of samples – they don’t exist for a single sample, but rather only as metrics that relate samples to each other. i.e. beta diversity =  patterns in community structure between samples

Since differences in sampling depths between samples can influence distance/dissimilarity metrics, we first need to somehow normalize the read depth across our samples. 

## Subsample

We will rarefy (random subsample with replacement) the read depth of the samples first (scale to the smallest library size).

[Case for not subsampling]( https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003531)

[Response blog for subsampling](https://www.polarmicrobes.org/how-i-learned-to-stop-worrying-and-love-subsampling-rarifying/)

Read depth is an artifact of a machine made by a company in San Diego, not anything about your samples or their biology. It is totally artifactual, and controlling for artifacts is critical in science. Subsampling randomly is the simplest way to control for this, and the question is whether this is the "best" way of controlling for it. See links above for alternative arguments about what the best way of controlling for this artifact is. 

A strong reason to subsample is to standardize effort. The bottom line is that in all experimental design you should not be comparing things to which you devote different effort in resolution. For instance, you don't sample one site once a week and another once a month if you want to compare the dynamics between the sites. You standardize effort.

With that said, the bigger your differential in mean (or median) read depth (reads/sample) between pre- and post-subsampling, the greater the "effect" on beta diversity. 

Examples:

- means reads before = 40k, mean reads after = 1k, big effect.
- mean reads before = 40k, mean reads after = 20k, small effect.
- mean reads before = 2k, mean reads after = 1k, small effect.


We will subsample  three ways (depths of 5000, minimum read depth of all samples, and no subsampling) and compare resulting patterns, inluding:

- How environmental patterns in alpha diversity change
- How absolute alpha diversity changes
- How patterns among samples change (compare distance matrices, or compare PERMANOVA results)

```{r}

ps_min <-  rarefy_even_depth(sub_ps, sample.size = min(sample_sums(sub_ps)), rngseed = 532898)
ps_5000 <-  rarefy_even_depth(sub_ps, sample.size = 5000, rngseed = 532898)

#mean read depth before and after random subsampling, larger differential = greater "effect" on beta diversity
mean(sample_sums(sub_ps)) #85856
min(sample_sums(sub_ps)) #9702


```

## Unconstrained Ordination

One of the best exploratory analyses for amplicon data is unconstrained ordinations. Here we will look at ordinations of our full community samples. We will rarfy the samples first (scale to the smallest library size).


### PCoA

```{r}
pcoa <- ordinate(ps_min, method = "PCoA", distance = "bray")
```

```{r echo = FALSE, warning = FALSE, message = FALSE, fig.height = 8, fig.width = 22, fig.align = "center"}

pcoa.plot <- plot_ordination(ps_min, pcoa,  title = "Bray Curtis PCoA") +
  geom_hline(yintercept = 0, linetype = 2) +
  geom_point(aes(fill = latitude, shape = Depth.Interval), alpha = 0.6, stroke = 2, size = 4) +
  scale_shape_manual(values = c(21, 22, 23)) +
  scale_fill_gradientn(colors = odv.colors) +
  custom_theme()  
#removing one of the plotting layers (there are points within points)
pcoa.plot$layers <- pcoa.plot$layers[-1]

pcoa.plot + 
  facet_grid(~Season) +
   guides(fill = guide_colorbar(title = "Latitude, ˚N"), shape = guide_legend(title = "Depth Interval"))
```

### NMDS

Let’s try an NMDS instead. For NMDS plots it’s important to set a seed since the starting positions of samples in the alogrithm is random.

Important: if you calculate your bray-curtis distance metric “in-line” it will perform a square root transformation and Wisconsin double standardization. If you don’t want this, you can calculate your bray-curtis distance separately


```{r}
set.seed(1)

# Ordinate
nmds <- ordinate(ps_min,, method = "NMDS",  distance = "bray") # stress = 0.09
```


```{r echo = FALSE, warning = FALSE, message = FALSE, fig.height = 8, fig.width = 22, fig.align = "center"}

nmds.plot <- plot_ordination(ps_min, nmds,  title = "NMDS") +
  geom_hline(yintercept = 0, linetype = 2) +
  geom_point(aes(fill = latitude, shape = Depth.Interval), alpha = 0.6, stroke = 2, size = 4) +
  scale_shape_manual(values = c(21, 22, 23)) +
  scale_fill_gradientn(colors = odv.colors) +
  custom_theme()  
#removing one of the plotting layers (there are points within points)
nmds.plot$layers <- nmds.plot$layers[-1]

nmds.plot + 
  facet_grid(~Season) +
   guides(fill = guide_colorbar(title = "Latitude, ˚N"), shape = guide_legend(title = "Depth Interval"))
```

NMDS plots attempt to show ordinal distances between samples as accurately as possible in two dimensions. It is important to report the stress of these plots, because a high stress value means that the algorithm had a hard time representing the distances between samples in 2 dimensions. The stress of this plot was good - it was .09 (generally anything below .2 is considered acceptable). The PCoA for this data was able to show ~46% variation in just two dimensions, so we may want to stick with that plot.

# Constrained Ordination

Above we used unconstrained ordinations (PCoA, NMDS) to show relationships between samples in low dimensions. We can use a constrained ordination to see how environmental variables are associated with these changes in community composition. We constrain the ordination axes to linear combinations of environmental variables. We then plot the environmental scores onto the ordination

```{r}
# Remove data points with missing metadata
ps_not_na <- ps_min %>%
  subset_samples(
    !is.na(mld) &
      !is.na(npp) & 
      !is.na(doc) &
      !is.na(n) & 
      !is.na(bc) & 
      !is.na(bcd) & 
      !is.na(o2) & 
      !is.na(fl) & 
      !is.na(sal) & 
      !is.na(temp)
  )

bray <- phyloseq::distance(ps_not_na, method = "bray")

# CAP ordinate
cap_ord <- ordinate(ps_not_na, method = "CAP", distance = bray, formula = ~ mld + doc + n  + bcd + o2 + fl + sal + bc + z + latitude + npp + temp
)
```



```{r echo = FALSE, warning = FALSE, message = FALSE, fig.height = 8, fig.width = 22, fig.align = "center"}

# CAP plot
cap.plot <- plot_ordination(ps_not_na, ordination = cap_ord,  axes = c(1,2)) + 
  geom_hline(yintercept = 0, linetype = 2) +
  geom_point(aes(fill = latitude, shape = Depth.Interval), alpha = 0.6, stroke = 2, size = 4) +
  scale_shape_manual(values = c(21, 22, 23)) +
  scale_fill_gradientn(colors = odv.colors) +
  custom_theme()  +
  facet_grid(~Season) +
  guides(fill = guide_colorbar(title = "Latitude, ˚N"), shape = guide_legend(title = "Depth Interval"))
#removing one of the plotting layers (there are points within points)

cap.plot$layers <- cap.plot$layers[-1]



# Now add the environmental variables as arrows
arrowmat <- vegan::scores(cap_ord, display = "bp")

# Add labels, make a data.frame
arrowdf <- data.frame(labels = rownames(arrowmat), arrowmat)

# Define the arrow aesthetic mapping
arrow_map <- aes(xend = CAP1, 
    yend = CAP2, 
    x = 0, 
    y = 0, 
    shape = NULL, 
    color = NULL, 
    label = labels)

label_map <- aes(x = 1.3 * CAP1, 
    y = 1.3 * CAP2, 
    shape = NULL, 
    color = NULL, 
    label = labels)

arrowhead = arrow(length = unit(0.02, "npc"))

# Make a new graphic
cap.plot + 
  geom_segment(
    mapping = arrow_map, 
    size = .5, 
    data = arrowdf, 
    color = "gray", 
    arrow = arrowhead
  ) + 
  geom_text(
    mapping = label_map, 
    size = 4,  
    data = arrowdf, 
    show.legend = FALSE
  )

```

Do a permutational ANOVA on constrained axes used in ordination

```{r}
anova(cap_ord)
```



# Custom Table

phyloseq objects can sometimes be hard to handle when you have a particular plot in mind that you want to make. to make it easier for us: we'll extract the relative abundance data from the object and merge them with the taxa and sample info data as a new dataframe


## Generate relative abundances

Our data currently shows the relative proportion of different sequences to the total number of gene copies recovered, so we'll normalize the gene copy number

```{r}
ps_std <- transform_sample_counts(sub_ps, function(x) x/sum(x))
#extract the relative abundance table and coerce into dataframe
ps_std.tab <- as(otu_table(ps_std), "matrix")
ps_std.df = as.data.frame(ps_std.tab) 
```

## Table with Rel Abund, Taxa, Sample Info 

Create a new table that combines relative abundances with the taxa table

```{r}
#first coerce the taxa table into a data frame
tax.df = as.data.frame(tax.tab) 
#then combine the data frames
custom.tab <- tax.df %>% 
  rownames_to_column(., var = "asv") %>% 
  left_join(., ps_std.df %>% rownames_to_column(., var = "asv")) %>% 
  #create a new index of that combines the  class, order, family, and genus values
  mutate(cofg = paste(Class, "_", Order,"_", Family, "_", Genus )) %>% 
  select(-c(asv:Genus)) %>% 
  select(cofg:everything()) 

#save the row names and then make them into the column names
colnames <- custom.tab[,1]

#transpose the dataframe so we can merge with the sample info table
t_custom.tab <-  as.data.frame(t(custom.tab[,-1]))
colnames(t_custom.tab) <- colnames

#merge
sweet.tab <- t_custom.tab %>% 
  rownames_to_column(., var = "sample") %>% 
  left_join(., sample.tab %>% rownames_to_column(., var = "sample")) %>% 
  select(sample, Cruise:bcd, everything()) %>% 
  arrange(CampCN, z)
```

## Save

```{r}
saveRDS(sweet.tab, "~/GITHUB/naames_multiday/Output/Custom_ASV_Table.rds")
saveRDS(sub_ps, "~/GITHUB/naames_multiday/Output/phyloseq_obj.rds")


```



















