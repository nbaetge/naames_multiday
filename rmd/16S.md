16S
================
Nicholas Baetge
8/13/2020

# Intro

Here, the NAAMES cast 16S sequences processed by Luis Bolanos in the
Giovannoni group are explored

``` r
library(tidyverse) 
library(rmarkdown)
library(knitr)
library(readxl)
library(data.table) 
library(scales)
library(zoo)
library(oce)
library(patchwork)
#rmarkdown tables
library(stargazer)
library(pander)
#stat tests
library(lmtest)
library(lmodel2)
library(rstatix)
library(ggpubr)
#for odv type plots
library(lubridate)
library(reshape2)
library(MBA)
library(mgcv)
#phyloseq
library(phyloseq)
library(RColorBrewer)
```

``` r
custom_theme <- function() {
  theme_test(base_size = 30) %+replace%
    theme(legend.position = "right",
          legend.spacing.x = unit(0.5,"cm"),
          legend.title = element_text(size = 14),
          legend.text = element_text(size = 14),
          legend.background = element_rect(fill = "transparent",colour = NA),
          legend.key = element_rect(fill = "transparent",colour = NA),
          panel.background = element_rect(fill = "transparent",colour = NA),
          plot.background = element_rect(fill = "transparent",colour = NA)) 
}

custom.colors <- c("AT39" = "#377EB8", "AT34" = "#4DAF4A", "AT38" = "#E41A1C", "AT32" = "#FF7F00", "Temperate" = "#A6CEE3", "Subpolar" = "#377EB8", "Subtropical" = "#FB9A99", "GS/Sargasso" = "#E41A1C", "Early Spring" = "#377EB8", "Late Spring" = "#4DAF4A","Early Autumn" = "#E41A1C", "Summer" = "#E41A1C", "Late Autumn" = "#FF7F00", "Gv2_2019" = "#377EB8", "WOA18_MN" = "#4DAF4A", "WOA18_AN" = "#E41A1C")

levels = c("GS/Sargasso", "Subtropical", "Temperate", "Subpolar",  "AT39-6", "AT34", "AT38", "AT32","South", "North", "Early Spring", "Late Spring","Early Autumn",  "Summer", "Late Autumn", "Gv2_2019", "WOA18_MN", "WOA18_AN","Nov", "Nov sd", "Dec", "Dec sd", "Jan", "Jan sd", "Feb", "Feb sd", "Mar", "Mar sd", "Apr", "Apr sd",  "Cruise", "ARGO", "5-100 m", "150-200 m", "300 m", "> 300 m", "0", "1", "2", "3")


bar.colors <- c("100 m" = "white", "CM" = "#4DAF4A",  "PAM" = "#377EB8")

odv.colors <- c("#feb483", "#d31f2a", "#ffc000", "#27ab19", "#0db5e6", "#7139fe", "#d16cfa")
```

# Import Data

``` r
count.tab <- read.table("~/GITHUB/naames_multiday/Input/16s/HetV1OTU.txt", header = T, row.names = 1, check.names = F)

tax.tab <- as.matrix(read.table("~/GITHUB/naames_multiday/Input/16s/HetV1TUtax.txt", header = T, row.names = 1, check.names = F, na.strings = "", sep = "\t"))


ctd <-  readRDS("~/GITHUB/naames_multiday/Input/ctd/deriv_naames_ctd.rds") %>%
              select(Cruise, Station, CampCN,  bin_depth, deriv_o2_umol_l, fl_mg_m3, ave_temp_c, ave_sal_psu) %>% 
              mutate(Cruise = ifelse(Cruise == "AT39", "AT39-6", Cruise)) %>% 
              rename(z = bin_depth,
                     o2 = deriv_o2_umol_l,
                     fl = fl_mg_m3,
                     temp = ave_temp_c,
                     sal = ave_sal_psu) 


npp <- read_rds("~/GITHUB/naames_multiday/Input/Z_resolved_model_NPP.rds") %>% 
  rename(z = depth,
         npp = NPP)

sample.tab <- read_rds("~/GITHUB/naames_multiday/Input/export_ms/processed_bf.2.2020.rds") %>% 
  select(Cruise:CampCN, Target_Z, DNA_ID) %>% 
  drop_na(DNA_ID) %>% 
  rename(z = Target_Z) %>% 
  left_join(., read_rds("~/GITHUB/naames_multiday/Output/processed_data.rds") %>%
              select(Cruise, Station, Date,  CampCN, mld, z,  doc, n, phyc, bc, bcd ) %>% 
              distinct() %>% 
              mutate_at(vars(phyc:bcd), function(x)(x/10^3))) %>% 
  mutate(`Depth Interval` = ifelse(z <= 100, "5-100 m", "150-200 m"),
         `Depth Interval` = ifelse(z == 300, "300 m", `Depth Interval`),
         `Depth Interval` = ifelse(z > 300, "> 300 m", `Depth Interval`)) %>% 
  select(Cruise:z, `Depth Interval`, everything()) %>% 
  left_join(., ctd) %>% 
  left_join(., npp) %>% 
  rename(latitude = Latitude) %>% 
  column_to_rownames(var = "DNA_ID") 
```

# Phyloseq Object

We need to create a phyloseq object that merges all three datasets.
Sometimes this doesn’t work beacuse of the format of the data files.
Make sure all the sample names between the sampleinfo.txt and
seqtab-nochimtaxa.txt are the same

``` r
OTU = otu_table(count.tab, taxa_are_rows = TRUE) 
TAX = tax_table(tax.tab)
SAM = sample_data(sample.tab)
ps = phyloseq(OTU,TAX,SAM) 

sample_data(ps)$`Depth.Interval` <- factor(sample_data(ps)$`Depth.Interval`, levels = levels)
sample_data(ps)$Season <- factor(sample_data(ps)$Season, levels = levels)
```

# Filter sequences

We will filter out chloroplasts and mitochondria, because we only
intended to amplify bacterial sequences. It’s good to check you don’t
have anything lurking in the taxonomy table.

``` r
sub_ps <- ps %>%
  subset_samples(z %in% c(5, 25, 50, 75, 100, 150, 200, 300)) %>%
  subset_taxa(
    Family  != "mitochondria" &
    Class   != "Chloroplast"
  )
```

# Sample Summary

As a first analysis, we will look at the distribution of read counts
from our
samples

<img src="16S_files/figure-gfm/unnamed-chunk-6-1.png" style="display: block; margin: auto;" />

``` r
# mean, max and min of sample read counts
smin <- min(sample_sums(sub_ps)) #3944
smean <- mean(sample_sums(sub_ps)) #85824.59
smax <- max(sample_sums(sub_ps)) #251468
```

# Stacked Barplots

Let’s make a stacked barplot of Phyla to get a sense of the community
composition in these samples.

Since this is not a quantitative analysis, and since we have more Phyla
in this dataset than we can reasonably distinguish colors, we will prune
out low abundance taxa and only include Phyla that contribute more than
2% of the relative abundance of each sample. Depending on your dataset
and the taxonomic level you are depicting, you can adjust this prune
parameter. In later analyses, we will of course included these taxa, but
for now they will just clutter our plot.

``` r
# melt to long format (for ggploting) 
# prune out phyla below 2% in each sample

sub_ps_phylum <- sub_ps %>%
  tax_glom(taxrank = "Phylum") %>%                     # agglomerate at phylum level
  transform_sample_counts(function(x) {x/sum(x)} ) %>% # Transform to rel. abundance
  psmelt() %>%                                         # Melt to long format
  filter(Abundance > 0.01) %>%                         # Filter out low abundance taxa
  arrange(Phylum)                                      # Sort data frame alphabetically by phylum
```

<img src="16S_files/figure-gfm/unnamed-chunk-9-1.png" style="display: block; margin: auto;" />
\# Beta Diversity

Beta diversity involves calculating metrics such as distances or
dissimilarities based on pairwise comparisons of samples – they don’t
exist for a single sample, but rather only as metrics that relate
samples to each other. i.e. beta diversity = patterns in community
structure between samples

Since differences in sampling depths between samples can influence
distance/dissimilarity metrics, we first need to somehow normalize the
read depth across our samples.

## Subsample

We will rarefy (random subsample with replacement) the read depth of the
samples first (scale to the smallest library size).

[Case for not
subsampling](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003531)

[Response blog for
subsampling](https://www.polarmicrobes.org/how-i-learned-to-stop-worrying-and-love-subsampling-rarifying/)

Read depth is an artifact of a machine made by a company in San Diego,
not anything about your samples or their biology. It is totally
artifactual, and controlling for artifacts is critical in science.
Subsampling randomly is the simplest way to control for this, and the
question is whether this is the “best” way of controlling for it. See
links above for alternative arguments about what the best way of
controlling for this artifact is.

A strong reason to subsample is to standardize effort. The bottom line
is that in all experimental design you should not be comparing things to
which you devote different effort in resolution. For instance, you don’t
sample one site once a week and another once a month if you want to
compare the dynamics between the sites. You standardize effort.

With that said, the bigger your differential in mean (or median) read
depth (reads/sample) between pre- and post-subsampling, the greater the
“effect” on beta diversity.

Examples:

  - means reads before = 40k, mean reads after = 1k, big effect.
  - mean reads before = 40k, mean reads after = 20k, small effect.
  - mean reads before = 2k, mean reads after = 1k, small effect.

We will subsample three ways (depths of 5000, minimum read depth of all
samples, and no subsampling) and compare resulting patterns, inluding:

  - How environmental patterns in alpha diversity change
  - How absolute alpha diversity changes
  - How patterns among samples change (compare distance matrices, or
    compare PERMANOVA
results)

<!-- end list -->

``` r
ps_min <-  rarefy_even_depth(sub_ps, sample.size = min(sample_sums(sub_ps)), rngseed = 532898)
```

    ## `set.seed(532898)` was used to initialize repeatable random subsampling.

    ## Please record this for your records so others can reproduce.

    ## Try `set.seed(532898); .Random.seed` for the full vector

    ## ...

    ## 2236OTUs were removed because they are no longer 
    ## present in any sample after random subsampling

    ## ...

``` r
ps_5000 <-  rarefy_even_depth(sub_ps, sample.size = 5000, rngseed = 532898)
```

    ## `set.seed(532898)` was used to initialize repeatable random subsampling.

    ## Please record this for your records so others can reproduce.

    ## Try `set.seed(532898); .Random.seed` for the full vector

    ## ...

    ## 2743OTUs were removed because they are no longer 
    ## present in any sample after random subsampling

    ## ...

``` r
#mean read depth before and after random subsampling, larger differential = greater "effect" on beta diversity
mean(sample_sums(sub_ps)) #85856
```

    ## [1] 85856.38

``` r
min(sample_sums(sub_ps)) #9702
```

    ## [1] 9702

## Unconstrained Ordination

One of the best exploratory analyses for amplicon data is unconstrained
ordinations. Here we will look at ordinations of our full community
samples. We will rarfy the samples first (scale to the smallest library
size).

### PCoA

``` r
pcoa <- ordinate(ps_min, method = "PCoA", distance = "bray")
```

<img src="16S_files/figure-gfm/unnamed-chunk-12-1.png" style="display: block; margin: auto;" />

### NMDS

Let’s try an NMDS instead. For NMDS plots it’s important to set a seed
since the starting positions of samples in the alogrithm is random.

Important: if you calculate your bray-curtis distance metric “in-line”
it will perform a square root transformation and Wisconsin double
standardization. If you don’t want this, you can calculate your
bray-curtis distance separately

``` r
set.seed(1)

# Ordinate
nmds <- ordinate(ps_min,, method = "NMDS",  distance = "bray") # stress = 0.09
```

    ## Square root transformation
    ## Wisconsin double standardization
    ## Run 0 stress 0.09046925 
    ## Run 1 stress 0.09046929 
    ## ... Procrustes: rmse 1.794898e-05  max resid 0.0001574416 
    ## ... Similar to previous best
    ## Run 2 stress 0.09833296 
    ## Run 3 stress 0.09833298 
    ## Run 4 stress 0.09046947 
    ## ... Procrustes: rmse 9.893541e-05  max resid 0.001008298 
    ## ... Similar to previous best
    ## Run 5 stress 0.1024095 
    ## Run 6 stress 0.09046942 
    ## ... Procrustes: rmse 9.226038e-05  max resid 0.0009969182 
    ## ... Similar to previous best
    ## Run 7 stress 0.09046926 
    ## ... Procrustes: rmse 1.604086e-05  max resid 0.0001534156 
    ## ... Similar to previous best
    ## Run 8 stress 0.09046924 
    ## ... New best solution
    ## ... Procrustes: rmse 1.756906e-05  max resid 0.0002521451 
    ## ... Similar to previous best
    ## Run 9 stress 0.09046926 
    ## ... Procrustes: rmse 1.757317e-05  max resid 0.0002487919 
    ## ... Similar to previous best
    ## Run 10 stress 0.1024095 
    ## Run 11 stress 0.09046926 
    ## ... Procrustes: rmse 1.775394e-05  max resid 0.0002238172 
    ## ... Similar to previous best
    ## Run 12 stress 0.09046925 
    ## ... Procrustes: rmse 1.728101e-05  max resid 0.0002137143 
    ## ... Similar to previous best
    ## Run 13 stress 0.09833306 
    ## Run 14 stress 0.09046942 
    ## ... Procrustes: rmse 8.790245e-05  max resid 0.000979484 
    ## ... Similar to previous best
    ## Run 15 stress 0.09046925 
    ## ... Procrustes: rmse 1.561472e-05  max resid 0.00022299 
    ## ... Similar to previous best
    ## Run 16 stress 0.09046924 
    ## ... New best solution
    ## ... Procrustes: rmse 1.038319e-05  max resid 0.0001477574 
    ## ... Similar to previous best
    ## Run 17 stress 0.09046924 
    ## ... Procrustes: rmse 4.024961e-06  max resid 4.785136e-05 
    ## ... Similar to previous best
    ## Run 18 stress 0.09046928 
    ## ... Procrustes: rmse 1.064839e-05  max resid 8.926098e-05 
    ## ... Similar to previous best
    ## Run 19 stress 0.09046925 
    ## ... Procrustes: rmse 6.357082e-06  max resid 6.339827e-05 
    ## ... Similar to previous best
    ## Run 20 stress 0.098333 
    ## *** Solution reached

<img src="16S_files/figure-gfm/unnamed-chunk-14-1.png" style="display: block; margin: auto;" />

NMDS plots attempt to show ordinal distances between samples as
accurately as possible in two dimensions. It is important to report the
stress of these plots, because a high stress value means that the
algorithm had a hard time representing the distances between samples in
2 dimensions. The stress of this plot was good - it was .09 (generally
anything below .2 is considered acceptable). The PCoA for this data was
able to show ~46% variation in just two dimensions, so we may want to
stick with that plot.

# Constrained Ordination

Above we used unconstrained ordinations (PCoA, NMDS) to show
relationships between samples in low dimensions. We can use a
constrained ordination to see how environmental variables are associated
with these changes in community composition. We constrain the ordination
axes to linear combinations of environmental variables. We then plot the
environmental scores onto the ordination

``` r
# Remove data points with missing metadata
ps_not_na <- ps_min %>%
  subset_samples(
    !is.na(mld) &
      !is.na(npp) & 
      !is.na(doc) &
      !is.na(n) & 
      !is.na(bc) & 
      !is.na(bcd) & 
      !is.na(o2) & 
      !is.na(fl) & 
      !is.na(sal) & 
      !is.na(temp)
  )

bray <- phyloseq::distance(ps_not_na, method = "bray")

# CAP ordinate
cap_ord <- ordinate(ps_not_na, method = "CAP", distance = bray, formula = ~ mld + doc + n  + bcd + o2 + fl + sal + bc + z + latitude + npp + temp
)
```

<img src="16S_files/figure-gfm/unnamed-chunk-16-1.png" style="display: block; margin: auto;" />

Do a permutational ANOVA on constrained axes used in ordination

``` r
anova(cap_ord)
```

    ## Permutation test for capscale under reduced model
    ## Permutation: free
    ## Number of permutations: 999
    ## 
    ## Model: capscale(formula = distance ~ mld + doc + n + bcd + o2 + fl + sal + bc + z + latitude + npp + temp, data = data)
    ##           Df SumOfSqs      F Pr(>F)    
    ## Model     12   25.001 16.758  0.001 ***
    ## Residual 149   18.524                  
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

# Custom Table

phyloseq objects can sometimes be hard to handle when you have a
particular plot in mind that you want to make. to make it easier for us:
we’ll extract the relative abundance data from the object and merge them
with the taxa and sample info data as a new dataframe

## Generate relative abundances

Our data currently shows the relative proportion of different sequences
to the total number of gene copies recovered, so we’ll normalize the
gene copy number

``` r
ps_std <- transform_sample_counts(sub_ps, function(x) x/sum(x))
#extract the relative abundance table and coerce into dataframe
ps_std.tab <- as(otu_table(ps_std), "matrix")
ps_std.df = as.data.frame(ps_std.tab) 
```

## Table with Rel Abund, Taxa, Sample Info

Create a new table that combines relative abundances with the taxa table

``` r
#first coerce the taxa table into a data frame
tax.df = as.data.frame(tax.tab) 
#then combine the data frames
custom.tab <- tax.df %>% 
  rownames_to_column(., var = "asv") %>% 
  left_join(., ps_std.df %>% rownames_to_column(., var = "asv")) %>% 
  #create a new index of that combines the  class, order, family, and genus values
  mutate(cofg = paste(Class, "_", Order,"_", Family, "_", Genus )) %>% 
  select(-c(asv:Genus)) %>% 
  select(cofg:everything()) 
```

    ## Joining, by = "asv"

    ## Warning in x:y: numerical expression has 247 elements: only the first used

``` r
#save the row names and then make them into the column names
colnames <- custom.tab[,1]

#transpose the dataframe so we can merge with the sample info table
t_custom.tab <-  as.data.frame(t(custom.tab[,-1]))
colnames(t_custom.tab) <- colnames

#merge
sweet.tab <- t_custom.tab %>% 
  rownames_to_column(., var = "sample") %>% 
  left_join(., sample.tab %>% rownames_to_column(., var = "sample")) %>% 
  select(sample, Cruise:bcd, everything()) %>% 
  arrange(CampCN, z)
```

    ## Warning: The `.data` argument of `add_column()` must have unique names as of tibble 3.0.0.
    ## Use `.name_repair = "minimal"`.
    ## This warning is displayed once every 8 hours.
    ## Call `lifecycle::last_warnings()` to see where this warning was generated.

    ## Joining, by = "sample"

## Save

``` r
saveRDS(sweet.tab, "~/GITHUB/naames_multiday/Output/Custom_ASV_Table.rds")
saveRDS(sub_ps, "~/GITHUB/naames_multiday/Output/phyloseq_obj.rds")
```
