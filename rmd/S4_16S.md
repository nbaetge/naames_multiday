S4\_16S
================
Nicholas Baetge
8/14/2020

# Intro

Here, the NAAMES cast 16S sequences from N2S4 are analyzed

``` r
library(tidyverse) 
library(rmarkdown)
library(knitr)
library(readxl)
library(data.table) 
library(scales)
library(lubridate)
library(patchwork)
#stat tests
library(lmtest)
library(lmodel2)
library(rstatix)
library(ggpubr)
#phyloseq
library(phyloseq)
library(RColorBrewer)
library(calecopal)
```

``` r
custom_theme <- function() {
  theme_test(base_size = 24) %+replace%
    theme(legend.position = "right",
          legend.spacing.x = unit(0.5,"cm"),
          legend.title = element_text(size = 14),
          legend.text = element_text(size = 14),
          legend.background = element_rect(fill = "transparent",colour = NA),
          legend.key = element_rect(fill = "transparent",colour = NA),
          panel.background = element_rect(fill = "transparent",colour = NA),
          plot.background = element_rect(fill = "transparent",colour = NA)) 
}

custom.colors <- c("AT39" = "#377EB8", "AT34" = "#4DAF4A", "AT38" = "#E41A1C", "AT32" = "#FF7F00", "Temperate" = "#A6CEE3", "Subpolar" = "#377EB8", "Subtropical" = "#FB9A99", "GS/Sargasso" = "#E41A1C", "Early Spring" = "#377EB8", "Late Spring" = "#4DAF4A","Early Autumn" = "#E41A1C", "Late Autumn" = "#FF7F00")

levels = c("GS/Sargasso", "Subtropical", "Temperate", "Subpolar",  "AT39-6", "AT34", "AT38", "AT32", "Early Spring", "Late Spring","Early Autumn",   "Late Autumn", "5-75 m", "100-200 m", "300 m", "> 300 m", "0", "1", "2", "3", "5 m", "25 m", "50 m", "75 m", "100 m", "150 m", "200 m" )

odv.colors <- c("#feb483", "#d31f2a", "#ffc000", "#27ab19", "#0db5e6", "#7139fe", "#d16cfa")
```

# Import Data

``` r
custom.tab <- readRDS("~/GITHUB/naames_multiday/Output/Custom_ASV_Table.rds") %>% 
  filter(Cruise == "AT34" & Station == 4 & z <= 200) %>% 
  mutate(time = ymd_hms(datetime),
         interv = interval(first(time), time),
         dur = as.duration(interv),
         days = round(as.numeric(dur, "days")),
         eddy = ifelse(Date != "2016-05-27" & Station != 6, "Core", "Outside")) %>% 
  select(Cruise:bcd, eddy, time:days, everything())

sub_ps <- readRDS("~/GITHUB/naames_multiday/Output/phyloseq_obj.rds") %>% 
  subset_samples(Cruise == "AT34" & Station == 4 & z <= 200)
```

## Add new sample data to phyloseq object

``` r
ctd <-  readRDS("~/GITHUB/naames_multiday/Input/ctd/deriv_naames_ctd.rds") %>%
              select(Cruise, Station, CampCN,  bin_depth, deriv_aou_umol_l, fl_mg_m3, ave_temp_c, ave_sal_psu, beamT_perc, ave_dens_kg_m3) %>% 
              mutate(Cruise = ifelse(Cruise == "AT39", "AT39-6", Cruise)) %>% 
              rename(z = bin_depth,
                     aou = deriv_aou_umol_l,
                     fl = fl_mg_m3,
                     temp = ave_temp_c,
                     sal = ave_sal_psu,
                     density = ave_dens_kg_m3,
                     beamT = beamT_perc) 


npp <- read_rds("~/GITHUB/naames_multiday/Input/Z_resolved_model_NPP.rds") %>% 
  rename(z = depth,
         npp = NPP)

new.sample.tab <- read_rds("~/GITHUB/naames_multiday/Input/export_ms/processed_bf.8.2020.rds") %>% 
  mutate(Station = ifelse(Station == "1A", 0, Station)) %>% 
  mutate_at(vars(Station), as.numeric) %>% 
  select(Cruise:CampCN, Target_Z, DNA_ID) %>% 
  drop_na(DNA_ID) %>% 
  rename(z = Target_Z) %>% 
  left_join(., read_rds("~/GITHUB/naames_multiday/Output/processed_data.rds") %>%
              select(Cruise, Station, Date,  CampCN, mld, z,  doc, n, phyc, bc, bcd, tdaa, Asp:Lys ) %>% 
              distinct() %>% 
              mutate_at(vars(phyc:bcd, tdaa:Lys), function(x)(x/10^3))) %>% 
  mutate(sample_depths = ifelse(z < 100, "5-75 m", "100-200 m"),
         sample_depths = ifelse(z == 300, "300 m", sample_depths),
         sample_depths = ifelse(z > 300, "> 300 m", sample_depths)) %>% 
  select(Cruise:z, sample_depths, everything()) %>% 
  left_join(., ctd) %>% 
  left_join(., npp) %>% 
  filter(Cruise == "AT34" & Station == 4 & z <= 200) %>% 
  mutate(time = ymd_hms(datetime),
         interv = interval(first(time), time),
         dur = as.duration(interv),
         days = round(as.numeric(dur, "days")),
         eddy = ifelse(Date != "2016-05-27" & Station != 6, "Core", "Outside")) %>% 
  select(Cruise:npp, eddy,  time:days) %>% 
  column_to_rownames(var = "DNA_ID") 
```

    ## Joining, by = c("Cruise", "Station", "Date", "CampCN", "z")

    ## Joining, by = c("Cruise", "Station", "CampCN", "z")

    ## Joining, by = c("Cruise", "Station", "Date", "z")

``` r
sample_data(sub_ps) <- new.sample.tab
```

# Beta Diversity

Beta diversity involves calculating metrics such as distances or
dissimilarities based on pairwise comparisons of samples – they don’t
exist for a single sample, but rather only as metrics that relate
samples to each other. i.e. beta diversity = patterns in community
structure between samples

Since differences in sampling depths between samples can influence
distance/dissimilarity metrics, we first need to somehow normalize the
read depth across our samples.

### Subsample

We will rarefy (random subsample with replacement) the read depth of the
samples first (scale to the smallest library size). This is consistent
with the analysis done by Bolanos et al, in prep.

[Case for not
subsampling](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003531)

[Response blog for
subsampling](https://www.polarmicrobes.org/how-i-learned-to-stop-worrying-and-love-subsampling-rarifying/)

Read depth is an artifact of a machine made by a company in San Diego,
not anything about your samples or their biology. It is totally
artifactual, and controlling for artifacts is critical in science.
Subsampling randomly is the simplest way to control for this, and the
question is whether this is the “best” way of controlling for it. See
links above for alternative arguments about what the best way of
controlling for this artifact is.

A strong reason to subsample is to standardize effort. The bottom line
is that in all experimental design you should not be comparing things to
which you devote different effort in resolution. For instance, you don’t
sample one site once a week and another once a month if you want to
compare the dynamics between the sites. You standardize effort.

With that said, the bigger your differential in mean (or median) read
depth (reads/sample) between pre- and post-subsampling, the greater the
“effect” on beta diversity.

Examples:

  - means reads before = 40k, mean reads after = 1k, big effect.
  - mean reads before = 40k, mean reads after = 20k, small effect.
  - mean reads before = 2k, mean reads after = 1k, small effect.

We will subsample three ways (depths of 5000, minimum read depth of all
samples, and no subsampling) and compare resulting patterns, inluding:

  - How environmental patterns in alpha diversity change
  - How absolute alpha diversity changes
  - How patterns among samples change (compare distance matrices, or
    compare PERMANOVA results)

## Sample Summary

We will first look at the distribution of read counts from our
samples

<img src="S4_16S_files/figure-gfm/unnamed-chunk-5-1.png" style="display: block; margin: auto;" />

``` r
#mean read depth before and after random subsampling, larger differential = greater "effect" on beta diversity
mean(sample_sums(sub_ps)) #120,302
```

    ## [1] 120302.4

``` r
min(sample_sums(sub_ps)) #30,887
```

    ## [1] 30887

``` r
ps_min <- rarefy_even_depth(sub_ps, sample.size = min(sample_sums(sub_ps)), rngseed = 532898)
```

    ## `set.seed(532898)` was used to initialize repeatable random subsampling.

    ## Please record this for your records so others can reproduce.

    ## Try `set.seed(532898); .Random.seed` for the full vector

    ## ...

    ## 9401OTUs were removed because they are no longer 
    ## present in any sample after random subsampling

    ## ...

``` r
ps_10k = rarefy_even_depth(sub_ps, sample.size = 10000, rngseed = 532898)
```

    ## `set.seed(532898)` was used to initialize repeatable random subsampling.

    ## Please record this for your records so others can reproduce.

    ## Try `set.seed(532898); .Random.seed` for the full vector

    ## ...

    ## 9734OTUs were removed because they are no longer 
    ## present in any sample after random subsampling

    ## ...

## Unconstrained Ordination

One of the best exploratory analyses for amplicon data is unconstrained
ordinations. Here we will look at ordinations of our full community
samples.

### PCoA

``` r
pcoa_all <- ordinate(sub_ps, method = "PCoA", distance = "bray")
pcoa_min <- ordinate(ps_min, method = "PCoA", distance = "bray")
pcoa_10k <- ordinate(ps_10k, method = "PCoA", distance = "bray")
```

<img src="S4_16S_files/figure-gfm/unnamed-chunk-8-1.png" style="display: block; margin: auto;" />

<img src="S4_16S_files/figure-gfm/unnamed-chunk-9-1.png" style="display: block; margin: auto;" />

<img src="S4_16S_files/figure-gfm/unnamed-chunk-10-1.png" style="display: block; margin: auto;" />

### NMDS

Let’s try an NMDS instead. For NMDS plots it’s important to set a seed
since the starting positions of samples in the alogrithm is random.

Important: if you calculate your bray-curtis distance metric “in-line”
it will perform a square root transformation and Wisconsin double
standardization. If you don’t want this, you can calculate your
bray-curtis distance separately

``` r
set.seed(1)

# Ordinate
nmds_all <- ordinate(sub_ps, method = "NMDS",  distance = "bray", trymax = 500) # stress = 0.08
```

    ## Square root transformation
    ## Wisconsin double standardization
    ## Run 0 stress 0.08587739 
    ## Run 1 stress 0.09365946 
    ## Run 2 stress 0.08585637 
    ## ... New best solution
    ## ... Procrustes: rmse 0.06687282  max resid 0.2238382 
    ## Run 3 stress 0.08441245 
    ## ... New best solution
    ## ... Procrustes: rmse 0.02049939  max resid 0.1028506 
    ## Run 4 stress 0.09748563 
    ## Run 5 stress 0.09487471 
    ## Run 6 stress 0.08587482 
    ## Run 7 stress 0.10228 
    ## Run 8 stress 0.09955912 
    ## Run 9 stress 0.08704408 
    ## Run 10 stress 0.0842598 
    ## ... New best solution
    ## ... Procrustes: rmse 0.0251504  max resid 0.09026776 
    ## Run 11 stress 0.08745301 
    ## Run 12 stress 0.08482158 
    ## Run 13 stress 0.1020127 
    ## Run 14 stress 0.3893336 
    ## Run 15 stress 0.0963034 
    ## Run 16 stress 0.08536051 
    ## Run 17 stress 0.1024618 
    ## Run 18 stress 0.09425531 
    ## Run 19 stress 0.08529377 
    ## Run 20 stress 0.09278251 
    ## Run 21 stress 0.09918021 
    ## Run 22 stress 0.104918 
    ## Run 23 stress 0.09391641 
    ## Run 24 stress 0.09686245 
    ## Run 25 stress 0.09678299 
    ## Run 26 stress 0.08398616 
    ## ... New best solution
    ## ... Procrustes: rmse 0.01002629  max resid 0.04005909 
    ## Run 27 stress 0.09786992 
    ## Run 28 stress 0.1000545 
    ## Run 29 stress 0.09654333 
    ## Run 30 stress 0.0846581 
    ## Run 31 stress 0.09960541 
    ## Run 32 stress 0.09647361 
    ## Run 33 stress 0.08584743 
    ## Run 34 stress 0.09634342 
    ## Run 35 stress 0.08608127 
    ## Run 36 stress 0.09647619 
    ## Run 37 stress 0.09756907 
    ## Run 38 stress 0.08441448 
    ## ... Procrustes: rmse 0.02671734  max resid 0.1064629 
    ## Run 39 stress 0.08434795 
    ## ... Procrustes: rmse 0.01134825  max resid 0.04011727 
    ## Run 40 stress 0.08398395 
    ## ... New best solution
    ## ... Procrustes: rmse 0.0006250098  max resid 0.002684128 
    ## ... Similar to previous best
    ## *** Solution reached

``` r
nmds_min <- ordinate(ps_min, method = "NMDS",  distance = "bray", trymax = 500) # stress = 0.08, no convergence i.e. two axes not sufficient to view data
```

    ## Square root transformation
    ## Wisconsin double standardization
    ## Run 0 stress 0.0821507 
    ## Run 1 stress 0.08347681 
    ## Run 2 stress 0.09577438 
    ## Run 3 stress 0.08267655 
    ## Run 4 stress 0.09798047 
    ## Run 5 stress 0.08285971 
    ## Run 6 stress 0.09599794 
    ## Run 7 stress 0.09829416 
    ## Run 8 stress 0.08317367 
    ## Run 9 stress 0.09976316 
    ## Run 10 stress 0.08402499 
    ## Run 11 stress 0.09914662 
    ## Run 12 stress 0.08117686 
    ## ... New best solution
    ## ... Procrustes: rmse 0.03608772  max resid 0.1546208 
    ## Run 13 stress 0.0975526 
    ## Run 14 stress 0.09861175 
    ## Run 15 stress 0.0993321 
    ## Run 16 stress 0.09495239 
    ## Run 17 stress 0.09349137 
    ## Run 18 stress 0.09923419 
    ## Run 19 stress 0.09938594 
    ## Run 20 stress 0.08735339 
    ## Run 21 stress 0.08317824 
    ## Run 22 stress 0.08267511 
    ## Run 23 stress 0.102241 
    ## Run 24 stress 0.09759886 
    ## Run 25 stress 0.08151866 
    ## ... Procrustes: rmse 0.01373961  max resid 0.06336647 
    ## Run 26 stress 0.08152113 
    ## ... Procrustes: rmse 0.0135416  max resid 0.06240874 
    ## Run 27 stress 0.09609244 
    ## Run 28 stress 0.0952348 
    ## Run 29 stress 0.08235297 
    ## Run 30 stress 0.09653202 
    ## Run 31 stress 0.0840273 
    ## Run 32 stress 0.08347659 
    ## Run 33 stress 0.100613 
    ## Run 34 stress 0.09613609 
    ## Run 35 stress 0.1011593 
    ## Run 36 stress 0.09413239 
    ## Run 37 stress 0.08744857 
    ## Run 38 stress 0.09815927 
    ## Run 39 stress 0.08214405 
    ## Run 40 stress 0.08285849 
    ## Run 41 stress 0.0826803 
    ## Run 42 stress 0.08748961 
    ## Run 43 stress 0.09626496 
    ## Run 44 stress 0.08347377 
    ## Run 45 stress 0.09531802 
    ## Run 46 stress 0.0966436 
    ## Run 47 stress 0.08268112 
    ## Run 48 stress 0.08348041 
    ## Run 49 stress 0.0834739 
    ## Run 50 stress 0.08250433 
    ## Run 51 stress 0.0828609 
    ## Run 52 stress 0.09754744 
    ## Run 53 stress 0.1035309 
    ## Run 54 stress 0.09893189 
    ## Run 55 stress 0.09348909 
    ## Run 56 stress 0.08430435 
    ## Run 57 stress 0.09470733 
    ## Run 58 stress 0.0934899 
    ## Run 59 stress 0.08317308 
    ## Run 60 stress 0.08605767 
    ## Run 61 stress 0.09391314 
    ## Run 62 stress 0.09784399 
    ## Run 63 stress 0.0845019 
    ## Run 64 stress 0.08316404 
    ## Run 65 stress 0.09609206 
    ## Run 66 stress 0.09932387 
    ## Run 67 stress 0.08214988 
    ## Run 68 stress 0.08203961 
    ## Run 69 stress 0.08286846 
    ## Run 70 stress 0.1042184 
    ## Run 71 stress 0.08214416 
    ## Run 72 stress 0.09886724 
    ## Run 73 stress 0.08811836 
    ## Run 74 stress 0.1000223 
    ## Run 75 stress 0.08267544 
    ## Run 76 stress 0.09349377 
    ## Run 77 stress 0.09672464 
    ## Run 78 stress 0.3894172 
    ## Run 79 stress 0.08402202 
    ## Run 80 stress 0.08220783 
    ## Run 81 stress 0.09930104 
    ## Run 82 stress 0.08450327 
    ## Run 83 stress 0.08348156 
    ## Run 84 stress 0.08203513 
    ## Run 85 stress 0.08347903 
    ## Run 86 stress 0.0952302 
    ## Run 87 stress 0.09522605 
    ## Run 88 stress 0.08616274 
    ## Run 89 stress 0.09474399 
    ## Run 90 stress 0.1040687 
    ## Run 91 stress 0.09434338 
    ## Run 92 stress 0.08431082 
    ## Run 93 stress 0.1002196 
    ## Run 94 stress 0.08267404 
    ## Run 95 stress 0.09830372 
    ## Run 96 stress 0.1014927 
    ## Run 97 stress 0.08347772 
    ## Run 98 stress 0.09374352 
    ## Run 99 stress 0.08118078 
    ## ... Procrustes: rmse 0.0006775759  max resid 0.002883589 
    ## ... Similar to previous best
    ## *** Solution reached

``` r
nmds_10k <- ordinate(ps_10k, method = "NMDS",  distance = "bray", trymax = 500) # stress = 0.09, no convergence
```

    ## Square root transformation
    ## Wisconsin double standardization
    ## Run 0 stress 0.09541011 
    ## Run 1 stress 0.09540513 
    ## ... New best solution
    ## ... Procrustes: rmse 0.0006454624  max resid 0.002668668 
    ## ... Similar to previous best
    ## Run 2 stress 0.09891141 
    ## Run 3 stress 0.1109804 
    ## Run 4 stress 0.09500067 
    ## ... New best solution
    ## ... Procrustes: rmse 0.03195274  max resid 0.1299343 
    ## Run 5 stress 0.117197 
    ## Run 6 stress 0.09539894 
    ## ... Procrustes: rmse 0.03077371  max resid 0.1276734 
    ## Run 7 stress 0.1205917 
    ## Run 8 stress 0.09666367 
    ## Run 9 stress 0.1140752 
    ## Run 10 stress 0.09665938 
    ## Run 11 stress 0.09500485 
    ## ... Procrustes: rmse 0.00301709  max resid 0.0127637 
    ## Run 12 stress 0.1195673 
    ## Run 13 stress 0.1112083 
    ## Run 14 stress 0.09540508 
    ## ... Procrustes: rmse 0.03197735  max resid 0.1297656 
    ## Run 15 stress 0.09674809 
    ## Run 16 stress 0.09666178 
    ## Run 17 stress 0.1065509 
    ## Run 18 stress 0.09890491 
    ## Run 19 stress 0.1136329 
    ## Run 20 stress 0.09888812 
    ## Run 21 stress 0.1145998 
    ## Run 22 stress 0.09891284 
    ## Run 23 stress 0.1086645 
    ## Run 24 stress 0.09888164 
    ## Run 25 stress 0.09888596 
    ## Run 26 stress 0.1065561 
    ## Run 27 stress 0.09888653 
    ## Run 28 stress 0.1139642 
    ## Run 29 stress 0.1023195 
    ## Run 30 stress 0.1140258 
    ## Run 31 stress 0.09674462 
    ## Run 32 stress 0.09681262 
    ## Run 33 stress 0.09499828 
    ## ... New best solution
    ## ... Procrustes: rmse 0.001600831  max resid 0.006593766 
    ## ... Similar to previous best
    ## *** Solution reached

<img src="S4_16S_files/figure-gfm/unnamed-chunk-12-1.png" style="display: block; margin: auto;" />

<img src="S4_16S_files/figure-gfm/unnamed-chunk-13-1.png" style="display: block; margin: auto;" />

<img src="S4_16S_files/figure-gfm/unnamed-chunk-14-1.png" style="display: block; margin: auto;" />

NMDS plots attempt to show ordinal distances between samples as
accurately as possible in two dimensions. It is important to report the
stress of these plots, because a high stress value means that the
algorithm had a hard time representing the distances between samples in
2 dimensions. The stress of all the plots was good (generally anything
below 0.2 is considered acceptable).

## Constrained Ordination

Above we used unconstrained ordinations (PCoA, NMDS) to show
relationships between samples in low dimensions. We can use a
constrained ordination to see how environmental variables are associated
with these changes in community composition. We constrain the ordination
axes to linear combinations of environmental variables. We then plot the
environmental scores onto the ordination

``` r
# Remove data points with missing metadata
ps_all_na.rm <- sub_ps %>%
  subset_samples(!is.na(mld) & !is.na(npp) & !is.na(doc) & !is.na(n) & !is.na(bc) & !is.na(bcd) & !is.na(aou) & !is.na(fl) & !is.na(sal) & !is.na(temp) & !is.na(density) & !is.na(beamT) )

ps_min_na.rm <- ps_min %>%
  subset_samples(!is.na(mld) & !is.na(npp) & !is.na(doc) & !is.na(n) & !is.na(bc) & !is.na(bcd) & !is.na(aou) & !is.na(fl) & !is.na(sal) & !is.na(temp) & !is.na(density) & !is.na(beamT) )

ps_10k_na.rm <- ps_10k %>%
  subset_samples(!is.na(mld) & !is.na(npp) & !is.na(doc) & !is.na(n) & !is.na(bc) & !is.na(bcd) & !is.na(aou) & !is.na(fl) & !is.na(sal) & !is.na(temp) & !is.na(density) & !is.na(beamT) )


bray_all <- phyloseq::distance(ps_all_na.rm, method = "bray")
bray_min <- phyloseq::distance(ps_min_na.rm, method = "bray")
bray_10k <- phyloseq::distance(ps_10k_na.rm, method = "bray")

# CAP ordinate
cap_ord_all <- ordinate(ps_all_na.rm, method = "CAP", distance = bray_all, formula = ~ mld + doc + n  + bcd + aou + fl + sal + bc + beamT +  npp + temp + days + z + density )
cap_ord_min <- ordinate(ps_min_na.rm, method = "CAP", distance = bray_min, formula = ~ mld + doc + n  + bcd + aou + fl + sal + bc + beamT +  npp + temp + days + z + density )
cap_ord_10k <- ordinate(ps_10k_na.rm, method = "CAP", distance = bray_10k, formula = ~ mld + doc + n  + bcd + aou + fl + sal + bc + beamT +  npp + temp + days + z + density )
```

<img src="S4_16S_files/figure-gfm/unnamed-chunk-16-1.png" style="display: block; margin: auto;" />

<img src="S4_16S_files/figure-gfm/unnamed-chunk-17-1.png" style="display: block; margin: auto;" />

<img src="S4_16S_files/figure-gfm/unnamed-chunk-18-1.png" style="display: block; margin: auto;" />

Do a permutational ANOVA on constrained axes used in ordination

``` r
anova(cap_ord_all) 
```

    ## Permutation test for capscale under reduced model
    ## Permutation: free
    ## Number of permutations: 999
    ## 
    ## Model: capscale(formula = distance ~ mld + doc + n + bcd + aou + fl + sal + bc + beamT + npp + temp + days + z + density, data = data)
    ##          Df SumOfSqs      F Pr(>F)
    ## Model    14  0.81875 1.3745  0.221
    ## Residual 13  0.55311

``` r
anova(cap_ord_min) 
```

    ## Permutation test for capscale under reduced model
    ## Permutation: free
    ## Number of permutations: 999
    ## 
    ## Model: capscale(formula = distance ~ mld + doc + n + bcd + aou + fl + sal + bc + beamT + npp + temp + days + z + density, data = data)
    ##          Df SumOfSqs      F Pr(>F)  
    ## Model    14  0.15206 1.3007  0.035 *
    ## Residual 13  0.10856                
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

``` r
anova(cap_ord_10k) 
```

    ## Permutation test for capscale under reduced model
    ## Permutation: free
    ## Number of permutations: 999
    ## 
    ## Model: capscale(formula = distance ~ mld + doc + n + bcd + aou + fl + sal + bc + beamT + npp + temp + days + z + density, data = data)
    ##          Df SumOfSqs      F Pr(>F)  
    ## Model    14  0.20088 1.1878  0.052 .
    ## Residual 13  0.15704                
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

# Betadisper and permutational ANOVA

Above, we performed beta diversity analyses on Bray-Curtis distances on
rarefied datasets that were then visualized using PCoA, NMDS, and CAP.
We can test if there are statistically significant differences between
sample groups using the betadisper and adonis functions of the vegan
package. Betadisper tests whether two or more groups are homogeneously
dispersed in relation to their species in studied samples. This test can
be done to see if one group has more compositional variance than
another. Moreover, homogeneity of dispersion among groups is very
advisable to have if you want to test if two or more groups have
different compositions, which is tested by adonis.

## Phyloseq to DESeq, distance matrix

To be able to run the stats, we first have to create a distance matrix
from our data. We’ll use the DESeq package to do so.

``` r
library(DESeq2)
library(vegan)
```

``` r
deseq_counts <- phyloseq_to_deseq2(ps_min, design = ~days) #the design argument is required but doesn't matter here
```

    ## converting counts to integer mode

    ##   the design formula contains one or more numeric variables with integer values,
    ##   specifying a model with increasing fold change for higher values.
    ##   did you mean for this to be a factor? if so, first convert
    ##   this variable to a factor using the factor() function

``` r
deseq_count_tab <- assay(deseq_counts) #extract the read count matrix
```

We’ll calculate euclidean
distances

``` r
#We can subset our data if we want to and calculate distances/run stats for only a subset of the group. The code below shows how, but we're not actually going to subset anything
subset_sample_IDs <-  row.names(new.sample.tab)[between(new.sample.tab$z, 0, 200)]

euc_dist <- dist(t(deseq_count_tab[ , colnames(deseq_count_tab) %in% subset_sample_IDs]))

sample_info_tab <- new.sample.tab[row.names(new.sample.tab) %in% subset_sample_IDs, ]
```

Betadisper first calculates the average distance of group members to the
group centroid in multivariate space (generated by a distance matrix).

Our first question is: Is the community composition from 5-200 m
different between each day of the station occupation?

In the function below: we are using the distance matrix to calculate the
multivariate dispersions (variances; average distance to centroids). We
then use group dispersions to perform an ANOVA test.

``` r
anova(betadisper(euc_dist, sample_info_tab$Date)) 
```

    ## Analysis of Variance Table
    ## 
    ## Response: Distances
    ##           Df Sum Sq Mean Sq F value Pr(>F)
    ## Groups     3 222201   74067  1.9437 0.1495
    ## Residuals 24 914566   38107

The ANOVA’s p-value is not significant meaning that group dispersions
are homogenous (“Null hypothesis of no difference in dispersion between
groups”)

Adonis analyzes and partitions sums of squares using distance matrices.
It can be seen as an ANOVA using distance matrices (analogous to MANOVA
– multivariate analysis of variance). Therefore, it is used to test if
two or more groups have similar compositions.

``` r
adonis(euc_dist~sample_info_tab$Date)
```

    ## 
    ## Call:
    ## adonis(formula = euc_dist ~ sample_info_tab$Date) 
    ## 
    ## Permutation: free
    ## Number of permutations: 999
    ## 
    ## Terms added sequentially (first to last)
    ## 
    ##                      Df SumsOfSqs MeanSqs F.Model      R2 Pr(>F)   
    ## sample_info_tab$Date  1   2122523 2122523  6.5158 0.20039  0.008 **
    ## Residuals            26   8469496  325750         0.79961          
    ## Total                27  10592019                 1.00000          
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Our results indicate that the community between each day has a highly
significantly different composition (p \< 0.01).

**So our groups (days) present homogeneity among group dispersions
(compositions vary similarly) while having significantly different
compositions.**

What about at each depth horizon (5-75 m, 100 - 200
m)?

### Surface 75 m

``` r
#We can subset our data if we want to and calculate distances/run stats for only a subset of the group. The code below shows how, but we're not actually going to subset anything
surf_sample_IDs <-  row.names(new.sample.tab)[between(new.sample.tab$z, 0, 75)]

surf_euc_dist <- dist(t(deseq_count_tab[ , colnames(deseq_count_tab) %in% surf_sample_IDs]))

surf_sample_info_tab <- new.sample.tab[row.names(new.sample.tab) %in% surf_sample_IDs, ]
```

``` r
anova(betadisper(surf_euc_dist, surf_sample_info_tab$Date)) 
```

    ## Analysis of Variance Table
    ## 
    ## Response: Distances
    ##           Df Sum Sq Mean Sq F value Pr(>F)
    ## Groups     3  37929   12643  0.2037 0.8919
    ## Residuals 12 744841   62070

``` r
adonis(surf_euc_dist~surf_sample_info_tab$Date)
```

    ## 
    ## Call:
    ## adonis(formula = surf_euc_dist ~ surf_sample_info_tab$Date) 
    ## 
    ## Permutation: free
    ## Number of permutations: 999
    ## 
    ## Terms added sequentially (first to last)
    ## 
    ##                           Df SumsOfSqs MeanSqs F.Model      R2 Pr(>F)
    ## surf_sample_info_tab$Date  1    485490  485490  1.6685 0.10649  0.183
    ## Residuals                 14   4073721  290980         0.89351       
    ## Total                     15   4559211                 1.00000

**Our groups (days) in the surface 75 m present homogeneity among group
dispersions (compositions vary similarly) and do not have significantly
different
compositions.**

### 100 - 200 m

``` r
#We can subset our data if we want to and calculate distances/run stats for only a subset of the group. The code below shows how, but we're not actually going to subset anything
deep_sample_IDs <-  row.names(new.sample.tab)[between(new.sample.tab$z, 100, 200)]

deep_euc_dist <- dist(t(deseq_count_tab[ , colnames(deseq_count_tab) %in% deep_sample_IDs]))

deep_sample_info_tab <- new.sample.tab[row.names(new.sample.tab) %in% deep_sample_IDs, ]
```

``` r
anova(betadisper(deep_euc_dist, deep_sample_info_tab$Date)) 
```

    ## Analysis of Variance Table
    ## 
    ## Response: Distances
    ##           Df Sum Sq Mean Sq F value Pr(>F)
    ## Groups     3 380695  126898  1.1597 0.3832
    ## Residuals  8 875393  109424

``` r
adonis(deep_euc_dist~deep_sample_info_tab$Date)
```

    ## 
    ## Call:
    ## adonis(formula = deep_euc_dist ~ deep_sample_info_tab$Date) 
    ## 
    ## Permutation: free
    ## Number of permutations: 999
    ## 
    ## Terms added sequentially (first to last)
    ## 
    ##                           Df SumsOfSqs MeanSqs F.Model      R2 Pr(>F)  
    ## deep_sample_info_tab$Date  1   2167340 2167340  5.8949 0.37087  0.017 *
    ## Residuals                 10   3676660  367666         0.62913         
    ## Total                     11   5843999                 1.00000         
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

**Our groups (days) in the 100-200 m depth horizon present homogeneity
among group dispersions (compositions vary similarly) and have
significantly different compositions.**

# Alpha Diversity

Generating estimates of alpha diversity for microbial communities isn’t
hard, but generating *trustworthy* estimates is hard
[problematic](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC93182/) no
matter what you do.

We will use the subsampled library, which retains estimates of the
species abundance of the real population while standardizing sampling
effort.

[subsampling and alpha diversity
paper](https://www.frontiersin.org/articles/10.3389/fmicb.2019.02407/full)

[Chao1: nonparametric estimation of minimum community
richness](https://www.jstor.org/stable/4615964?seq=1#metadata_info_tab_contents)

Difference in the alpha diversity indexes among conditions were tested
using Kruskal-Wallis test followed by pairwise Wilcoxon tests; p \< 0.05
was considered the threshold significance for a difference between
conditions.

``` r
richness <- estimate_richness(ps_min, measures = c("Chao1", "Shannon")) %>% 
  rownames_to_column(., var = "DNA_ID") %>% 
  mutate_at(vars(DNA_ID), str_replace_all, pattern = "NAAMES2.", "NAAMES2-")
```

Let’s add the sample metadata into this
dataframe

``` r
alphadiv <- left_join(richness, new.sample.tab %>% rownames_to_column(., var = "DNA_ID")) 
```

    ## Joining, by = "DNA_ID"

<img src="S4_16S_files/figure-gfm/unnamed-chunk-38-1.png" style="display: block; margin: auto;" />

These are not interpretable as “real” numbers of anything (due to the
nature of amplicon data), but they can still be useful as relative
metrics of comparison. If Chao1 richness goes up, but Shannon diversity
goes down, it indicates that the sample may have more ASVs but is
dominated by a few of them.

**In this case, though our groups (days) showed significantly different
compositions, there weren’t significant changes in overall diversity or
richness. So it seems like the community shift wasn’t driven by the
emergence or disappearance of new ASVs, but rather the change in
relative proportion of existing ASVs. With that said, our grouping of
sample depths and box plots likely masks dynamics that we can kind of
see in the depth profile **

# DESeq2

Which taxa were important? Which taxa were contributing to thechange in
community compositon?

**Note: Recovered 16S rRNA gene copy numbers do not equal organism
abundance.**

That said, we can perform differential abundance testing to test for
which representative sequences have significantly different copy-number
counts between samples using the DESeq1 package, which allows testing
for differential abundance using negative binomial generalized linear
models.

``` r
# if (!requireNamespace("BiocManager", quietly = TRUE))
#     install.packages("BiocManager")
# 
# BiocManager::install("DESeq2")
```

``` r
deseq <-  DESeq(deseq_counts, test = "Wald", fitType = "local")
```

    ## estimating size factors

    ## estimating dispersions

    ## gene-wise dispersion estimates

    ## mean-dispersion relationship

    ## final dispersion estimates

    ## fitting model and testing

    ## -- replacing outliers and refitting for 28 genes
    ## -- DESeq argument 'minReplicatesForReplace' = 7 
    ## -- original counts are preserved in counts(dds)

    ## estimating dispersions

    ## fitting model and testing

We can save the results to a table with the DESeq results command,
retaining all results for further exploration (i.e., cooksCutoff = F).
Significant shifts are identified as those with a Benjamini–Hochberg
adjusted p-value \< 0.05

``` r
res <- results(deseq, cooksCutoff = FALSE)
alpha <- 0.05
sigtab <-  res[which(res$padj < alpha), ]
sigtab <-  cbind(as(sigtab, "data.frame"), as(tax_table(ps_min)[rownames(sigtab), ], "matrix"))
head(sigtab)
```

    ##      baseMean log2FoldChange      lfcSE     stat       pvalue         padj
    ## N1  5220.7704     0.10519446 0.02367212 4.443812 8.837887e-06 0.0004662561
    ## N5  1082.7810     0.10026286 0.02015917 4.973560 6.573442e-07 0.0001505318
    ## N7   599.5069     0.10407143 0.02405575 4.326261 1.516619e-05 0.0005788429
    ## N11  489.0538     0.08709817 0.02374652 3.667829 2.446182e-04 0.0062241748
    ## N13  468.0839     0.13780647 0.03005872 4.584575 4.549102e-06 0.0003472481
    ## N15  374.8822     0.07505169 0.02566902 2.923824 3.457604e-03 0.0465759538
    ##      Kingdom         Phylum               Class       Order   Family Genus
    ## N1  Bacteria Proteobacteria Alphaproteobacteria SAR11_clade SAR11_Ia    NA
    ## N5  Bacteria Proteobacteria Alphaproteobacteria SAR11_clade SAR11_Ia    NA
    ## N7  Bacteria Proteobacteria Alphaproteobacteria SAR11_clade SAR11_Ia    NA
    ## N11 Bacteria Proteobacteria Alphaproteobacteria SAR11_clade SAR11_Ia    NA
    ## N13 Bacteria Proteobacteria Alphaproteobacteria SAR11_clade SAR11_Ia    NA
    ## N15 Bacteria Proteobacteria Alphaproteobacteria SAR11_clade SAR11_Ia    NA

``` r
dim(sigtab)
```

    ## [1] 17 12

### Surface 75 m

``` r
surf_ps_min <- subset_samples(ps_min, between(z, 0, 75))
surf_deseq_counts <- phyloseq_to_deseq2(surf_ps_min, design = ~days) 
```

    ## converting counts to integer mode

    ##   the design formula contains one or more numeric variables with integer values,
    ##   specifying a model with increasing fold change for higher values.
    ##   did you mean for this to be a factor? if so, first convert
    ##   this variable to a factor using the factor() function

``` r
surf_deseq <-  DESeq(surf_deseq_counts, test = "Wald", fitType = "local")
```

    ## estimating size factors

    ## estimating dispersions

    ## gene-wise dispersion estimates

    ## mean-dispersion relationship

    ## final dispersion estimates

    ## fitting model and testing

``` r
surf_res <- results(surf_deseq, cooksCutoff = FALSE)
alpha <- 0.05
surf_sigtab <-  surf_res[which(surf_res$padj < alpha), ]
surf_sigtab <-  cbind(as(surf_sigtab, "data.frame"), as(tax_table(surf_ps_min)[rownames(surf_sigtab), ], "matrix"))
head(surf_sigtab)
```

    ##      baseMean log2FoldChange      lfcSE      stat       pvalue         padj
    ## N5  1080.4377     0.08807194 0.02741350  3.212721 1.314838e-03 0.0336927332
    ## N7   596.1324     0.10173487 0.02874217  3.539569 4.007813e-04 0.0186995390
    ## N13  456.3986     0.12840377 0.03663128  3.505304 4.560863e-04 0.0186995390
    ## N27  409.8172     0.11467576 0.03510585  3.266571 1.088583e-03 0.0318799350
    ## N31  517.0236    -0.13732357 0.04061486 -3.381116 7.219194e-04 0.0246655797
    ## N79  251.4713     0.18223650 0.03757118  4.850434 1.231919e-06 0.0001262717
    ##      Kingdom         Phylum               Class             Order
    ## N5  Bacteria Proteobacteria Alphaproteobacteria       SAR11_clade
    ## N7  Bacteria Proteobacteria Alphaproteobacteria       SAR11_clade
    ## N13 Bacteria Proteobacteria Alphaproteobacteria       SAR11_clade
    ## N27 Bacteria Proteobacteria Alphaproteobacteria   Rhodobacterales
    ## N31 Bacteria Proteobacteria Gammaproteobacteria Oceanospirillales
    ## N79 Bacteria  Bacteroidetes      Flavobacteriia  Flavobacteriales
    ##                 Family           Genus
    ## N5            SAR11_Ia              NA
    ## N7            SAR11_Ia              NA
    ## N13           SAR11_Ia              NA
    ## N27   Rhodobacteraceae              NA
    ## N31 Oceanospirillaceae Pseudospirillum
    ## N79   NS9_marine_group              NA

### 100 - 200 m

``` r
deep_ps_min <- subset_samples(ps_min, between(z, 100, 200))
deep_deseq_counts <- phyloseq_to_deseq2(deep_ps_min, design = ~days) 
```

    ## converting counts to integer mode

    ##   the design formula contains one or more numeric variables with integer values,
    ##   specifying a model with increasing fold change for higher values.
    ##   did you mean for this to be a factor? if so, first convert
    ##   this variable to a factor using the factor() function

``` r
deep_deseq <-  DESeq(deep_deseq_counts, test = "Wald", fitType = "local")
```

    ## estimating size factors

    ## estimating dispersions

    ## gene-wise dispersion estimates

    ## mean-dispersion relationship

    ## final dispersion estimates

    ## fitting model and testing

``` r
deep_res <- results(deep_deseq, cooksCutoff = FALSE)
alpha <- 0.05
deep_sigtab <-  deep_res[which(deep_res$padj < alpha), ]
deep_sigtab <-  cbind(as(deep_sigtab, "data.frame"), as(tax_table(deep_ps_min)[rownames(deep_sigtab), ], "matrix"))
head(deep_sigtab)
```

    ##         baseMean log2FoldChange      lfcSE      stat       pvalue       padj
    ## N5   1074.375588      0.1268899 0.03412752  3.718112 2.007175e-04 0.02726891
    ## N11   485.863100      0.1483928 0.03947708  3.758962 1.706200e-04 0.02704328
    ## N61   195.753345      0.2086619 0.05522574  3.778346 1.578735e-04 0.02704328
    ## N130   39.711693      0.4352533 0.10992470  3.959558 7.508846e-05 0.02380304
    ## N225   82.883092     -0.3393049 0.07860397 -4.316638 1.584234e-05 0.01386796
    ## N661    8.425747      1.5859263 0.37941819  4.179890 2.916500e-05 0.01386796
    ##       Kingdom         Phylum               Class           Order
    ## N5   Bacteria Proteobacteria Alphaproteobacteria     SAR11_clade
    ## N11  Bacteria Proteobacteria Alphaproteobacteria     SAR11_clade
    ## N61  Bacteria Proteobacteria Alphaproteobacteria     SAR11_clade
    ## N130 Bacteria Proteobacteria Alphaproteobacteria     SAR11_clade
    ## N225 Bacteria Proteobacteria Gammaproteobacteria Cellvibrionales
    ## N661 Bacteria Proteobacteria Gammaproteobacteria Alteromonadales
    ##                      Family             Genus
    ## N5                 SAR11_Ia                NA
    ## N11                SAR11_Ia                NA
    ## N61                SAR11_Ia                NA
    ## N130               SAR11_Ia                NA
    ## N225         Porticoccaceae       SAR92_clade
    ## N661 Pseudoalteromonadaceae Pseudoalteromonas

<img src="S4_16S_files/figure-gfm/unnamed-chunk-50-1.png" style="display: block; margin: auto;" />

# Custom Table

## Generate relative abundances

Our data currently shows the relative proportion of different sequences
to the total number of gene copies recovered, so we’ll normalize the
gene copy number

``` r
ps_std <- transform_sample_counts(ps_min, function(x) x/sum(x))
#extract the relative abundance table and coerce into dataframe
ps_std.tab <- as(otu_table(ps_std), "matrix")
ps_std.df = as.data.frame(ps_std.tab) 
```

## Table with Rel Abund, Taxa, Sample Info

Create a new table that combines relative abundances with the taxa
table

``` r
tax.tab <- as.matrix(read.table("~/GITHUB/naames_multiday/Input/16s/HetV1TUtax.txt", header = T, row.names = 1, check.names = F, na.strings = "", sep = "\t"))

#first coerce the taxa table into a data frame
tax.df = as.data.frame(tax.tab) 
#then combine the data frames
custom.tab <- tax.df %>% 
  rownames_to_column(., var = "asv") %>% 
  left_join(., ps_std.df %>% rownames_to_column(., var = "asv")) %>% 
  #create a new index of that combines the  class, order, family, and genus values
  mutate(#pcofg = paste(Phylum, "_", Class, "_", Order,"_", Family, "_", Genus),
         # pcof = paste(Phylum, "_", Class, "_", Order,"_", Family,),
         pco = paste(Phylum, "_", Class, "_", Order)) %>% 
  select(-c(asv:Genus)) %>% 
  # select(pcof,everything()) %>% 
  # group_by(pcof) %>% 
  select(pco,everything()) %>% 
  group_by(pco) %>% 
  summarise_at(vars(contains("NAAMES2")), sum, na.rm = T) %>% 
  ungroup()
```

    ## Joining, by = "asv"

``` r
#save the row names and then make them into the column names
colnames <- custom.tab[,1] 

#transpose the dataframe so we can merge with the sample info table
t_custom.tab <-  as.data.frame(t(custom.tab[,-1]))
# colnames(t_custom.tab) <- colnames$pcof
colnames(t_custom.tab) <- colnames$pco

#merge
sweet.tab <- t_custom.tab %>% 
  rownames_to_column(., var = "sample") %>% 
  left_join(., new.sample.tab %>% rownames_to_column(., var = "sample")) %>% 
  select(sample, Cruise:bcd, aou:days, tdaa:Lys, everything()) %>% 
  arrange(CampCN, z)
```

    ## Joining, by = "sample"

``` r
relabund <- sweet.tab %>% 
  select(-c(sample:Lys)) %>% 
  #remove groups that are completely absent
  .[ , colSums(.) > 0] %>% 
  #arrange by biggest contributors
  .[, order(colSums(-.))] %>% 
  bind_cols(sweet.tab %>% select(sample:Lys), .)
```

# Custom Z-score Table (Top 10 taxa)

``` r
#calculate z-scores (value - mean / sd)

value.relabund <- relabund %>% 
  select(Date, z, `Proteobacteria _ Alphaproteobacteria _ SAR11_clade`:`Proteobacteria _ Gammaproteobacteria _ Salinisphaerales`)

mean.relabund <- relabund  %>% 
  select(Date, z, `Proteobacteria _ Alphaproteobacteria _ SAR11_clade`:`Proteobacteria _ Gammaproteobacteria _ Salinisphaerales`) %>%
 group_by(z) %>%
  summarise_at(vars(`Proteobacteria _ Alphaproteobacteria _ SAR11_clade`:`Proteobacteria _ Gammaproteobacteria _ Salinisphaerales`), mean, na.rm = T) %>% 
  ungroup() %>%
  setNames(paste0(names(.), ".ave" )) %>% 
  mutate(z = z.ave) %>% 
  select(z, everything(), -z.ave)

sd.relabund <- relabund  %>% 
  select(Date, z, `Proteobacteria _ Alphaproteobacteria _ SAR11_clade`:`Proteobacteria _ Gammaproteobacteria _ Salinisphaerales`) %>%
  group_by(z) %>% 
  summarise_at(vars(`Proteobacteria _ Alphaproteobacteria _ SAR11_clade`:`Proteobacteria _ Gammaproteobacteria _ Salinisphaerales`), sd, na.rm = T) %>% 
  ungroup() %>% 
  setNames(paste0(names(.), ".sd" )) %>% 
  mutate(z = z.sd) %>% 
  select(z, everything(), -z.sd)


 
zscore <- left_join(value.relabund, mean.relabund, by = "z") %>% 
   pivot_longer(cols = -c(Date,z)) %>% 
   arrange(Date, z, name) %>% 
   mutate(name = gsub(".ave", "", name)) %>% 
   group_by(Date, z, name) %>% 
   mutate(diff = dplyr::first(value) - dplyr::last(value)) %>% 
   select(-value) %>% 
   distinct() %>% 
   pivot_wider(names_from = name, values_from = diff) %>% 
   ungroup() %>% 
   left_join(., sd.relabund, by = "z") %>% 
   pivot_longer(cols = -c(Date,z)) %>% 
   arrange(Date, z, name) %>% 
    mutate(name = gsub(".sd", "", name)) %>% 
   group_by(Date, z, name) %>% 
   mutate(zscore = dplyr::first(value)/dplyr::last(value)) %>% 
   select(-value) %>% 
   distinct() %>% 
   pivot_wider(names_from = name, values_from = zscore) 
```

# Heatmaps

<img src="S4_16S_files/figure-gfm/unnamed-chunk-56-1.png" style="display: block; margin: auto;" />

# ASV Depth Profiles

<img src="S4_16S_files/figure-gfm/unnamed-chunk-69-1.png" style="display: block; margin: auto;" />

# Stacked Barplots

Let’s make a stacked barplot of Phyla to get a sense of the community
composition in these samples.

Since this is not a quantitative analysis, and since we have more Phyla
in this dataset than we can reasonably distinguish colors, we will prune
out low abundance taxa and only include Orders that contribute more than
0.5% of the relative abundance of each sample. Depending on your dataset
and the taxonomic level you are depicting, you can adjust this prune
parameter. In later analyses, we will of course included these taxa, but
for now they will just clutter our
plot.

<img src="S4_16S_files/figure-gfm/unnamed-chunk-70-1.png" style="display: block; margin: auto;" />
